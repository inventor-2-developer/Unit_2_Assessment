{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waA-CPPLFP96"
   },
   "source": [
    "# Unit 2 Assessment: Big Data and Spark\n",
    "----\n",
    "\n",
    "### Overview\n",
    "\n",
    "The Unit 2 assessment covers Big Data, Spark SQL, PySpark, PyTest, and Great Expectations.  All of the questions for this assessment are contained in the `Unit_2_Assessment_unsolved.ipynb` Jupyter Notebook file. **The assessment is worth 50 points.**\n",
    "\n",
    "### Files\n",
    "\n",
    "Use the following link to download the assessment instructions and Jupyter Notebook file.\n",
    "\n",
    "[Download the Unit 2 Assessment resources](https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/Unit_2_Assessment.zip)\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Keep the following mind while working on the assessment: \n",
    "\n",
    "* Remember that this is an individual assessment&mdash;you may not work with your classmates. However, you are free to consult your course notes and activities to help you answer the questions. \n",
    "\n",
    "* Although this assessment is delivered in a Jupyter Notebook, we recommend that make a copy of the `Unit_2_Assessment_unsolved.ipynb` file and upload into Google Colab. \n",
    "\n",
    "    > **Note:** If your answers are not clearly identified, you may receive a score of “0” for that question. \n",
    "\n",
    "* When you are ready to submit your assessment, rename the Google Colab notebook file with your last name. For example, `Unit_2_Assessment_<your_last_name>.ipynb`. Please do not clear your outputs if you have written code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igYv6X7lDipu"
   },
   "source": [
    "## Question 1\n",
    "\n",
    "- **3 points**\n",
    "\n",
    "How do you display the schema of a Spark DataFrame?\n",
    "\n",
    "a. `.showSchema()`\n",
    "\n",
    "b. `.displaySchema()`\n",
    "\n",
    "c. `.printSchema()`\n",
    "\n",
    "d.`.schema().show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sm6P_d7FDteo"
   },
   "source": [
    "## Question 2\n",
    "\n",
    "- **3 points**\n",
    "\n",
    "How do you add a new column named, \"half_price\" that is half the price of the \"price\" column in a Spark DataFrame? \n",
    "\n",
    "a. `df.withColumn('half_price',df['price']/2)`\n",
    "\n",
    "b. `df.newColumn('half_price', df['price']/2)`\n",
    "\n",
    "c. `df.Column('half_price', df['price']/2)`\n",
    "\n",
    "d.  `df.withColumn('half_price', ['price']/2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6aInLH5D2C4"
   },
   "source": [
    "## Question 3\n",
    "\n",
    "- **3 points**\n",
    "\n",
    "\n",
    "How do you convert a PySpark DataFrame, `df` to a Pandas DataFrame after you use `import pandas as pd`?\n",
    "\n",
    "a. `df = pd.toPandas()`\n",
    "\n",
    "b. `pandas_df = pd.df.toPandas()`\n",
    "\n",
    "c. `pandas_df = df.toPandas()`\n",
    "\n",
    "d. `df = pd.df.toPandas()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rweor-nWEXQX"
   },
   "source": [
    "## Question 4\n",
    "\n",
    "- **8 points**\n",
    "\n",
    "Read in the [new vehicle](https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/new-vehicles.csv) dataset into Spark DataFrame and create a group by object that shows the total number of the all the vehicles in each \"New_Vehicle_Category\", then answer the following question. \n",
    "\n",
    "**How many passenger cars are there?**\n",
    "\n",
    "  - **Hint:** You will have to change the \"Count\" column to an integer.\n",
    "\n",
    "a. 230,220\n",
    "\n",
    "b. 47,425\n",
    "\n",
    "c. 2,254\n",
    "\n",
    "d. 397,182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruAOJ0BbE6dd"
   },
   "outputs": [],
   "source": [
    "# Activate Spark in our Colab notebook.\n",
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example: 'spark-3.2.2'\n",
    "spark_version = 'spark-3.2.2'\n",
    "# spark_version = 'spark-3.<enter version>'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.2.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3.2\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqCADKk-E9XF"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Vehicles\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVW7jBoKUeve"
   },
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/new-vehicles.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "vehicle_df = spark.read.csv(SparkFiles.get(\"new-vehicles.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "vehicle_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVgqBMJKEqJp"
   },
   "source": [
    "## Question 5\n",
    "\n",
    "- **5 points**\n",
    "\n",
    "Using the [new vehicle](https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/new-vehicles.csv) dataset create a temporary view of the Spark DataFrame. Use SQL to return the number of each vehicle in descending order, then answer the following question.\n",
    "\n",
    "Which make, model, and year has the most vehicles?\n",
    "\n",
    "a. 2009 Honda Civic\n",
    "\n",
    "b. 2010 Toyota Camry\n",
    "\n",
    "c. 2010 Toyota Corolla\n",
    "\n",
    "d. 2009 Honda Accord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwIhrrWlUevj"
   },
   "outputs": [],
   "source": [
    "# Create a temporary view of the vehicle_df.  \n",
    "vehicle_df2.createOrReplaceTempView('vehicles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyqfI6_aPy5i"
   },
   "source": [
    "## Question 6\n",
    "\n",
    "- **6 points**\n",
    "\n",
    "Read in the [USA income distribution](https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/income_distribution.csv) dataset into Spark DataFrame, create a temporary view, and write a query to return the number of households in descending order where the income level is less than $10,000 for the year 2016, then answer the following question. \n",
    "\n",
    "What state has the most number of households where the income in less than $10,000?\n",
    "\n",
    "  - **Hint:** You will have to cast the \"Percent_of_Total_Household\" column to a float and the \"Number_of_Households\" to an integer. \n",
    "\n",
    "a. New York\n",
    "\n",
    "b. California\n",
    "\n",
    "c. Texas\n",
    "\n",
    "d. Puerto Rico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIEN0qoVUevs"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/income_distribution.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "income_df = spark.read.csv(SparkFiles.get(\"income_distribution.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "income_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xffTbxR2K0XR"
   },
   "outputs": [],
   "source": [
    "income_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oq7c85HFK7EM"
   },
   "outputs": [],
   "source": [
    "# Convert the Percent_of_Total_Households to a float and Number_of_Households to an integer.\n",
    "income_df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vq-kfT6yUewG"
   },
   "source": [
    "## Question 7. \n",
    "\n",
    "- **3 points**\n",
    "\n",
    "If you have a dataset of 1 Tb what is the best option to increase query execution time? \n",
    "\n",
    "a. Create a temporary view, cache the temporary view and rerun the query. \n",
    "\n",
    "b. Write the data to a parquet format and rerun the query. \n",
    "\n",
    "c. Create a temporary view and rerun the query.\n",
    "\n",
    "d. Store in a AWS S3 bucket and query the table on Googl Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaZ0jrCSaMr-"
   },
   "source": [
    "## Question 8\n",
    "\n",
    "- **3 points**\n",
    "\n",
    "How do you check that a temporary view table. called, \"flight_delays\", is cached? \n",
    "\n",
    "a. `spark.temporaryView.isCached(\"flight_delays\")`\n",
    "\n",
    "b. `spark.temporaryView()(\"flight_delays\").isCached()`\n",
    "\n",
    "c. `spark.catalog.isCached(\"flight_delays\")`\n",
    "\n",
    "d. `spark.catalog.temporaryView(\"flight_delays\").isCached()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uO2MMkR1Uewm"
   },
   "source": [
    "## Question 9\n",
    "\n",
    "- **8 points**\n",
    "\n",
    "Read in the [SP_500_5yr_stock_data.csv](\"https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/SP_500_5yr_stock_data.csv) dataset into Pandas DataFrame. \n",
    "\n",
    "What is the assert statement will pass for the 1,258 rows in the DataFrame? \n",
    "\n",
    "a. `assert len(df.count()) == 1258`\n",
    "\n",
    "b. `assert len(df.index()) == 1258`\n",
    "\n",
    "c. `assert len(df.index) == 1258`\n",
    "\n",
    "d. `assert len(df.count) == 1258`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByBCXLBAhX00"
   },
   "outputs": [],
   "source": [
    "# Install pytest and pytest-sugar to make our output look nice.\n",
    "!pip install -q pytest pytest-sugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Hjv3hBphemZ"
   },
   "outputs": [],
   "source": [
    "# Create and navigate to the tests directory.\n",
    "from pathlib import Path\n",
    "if Path.cwd().name != 'tests':\n",
    "    %mkdir tests\n",
    "    %cd tests\n",
    "# Show the current working directory. \n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgEpdizPhjy0"
   },
   "outputs": [],
   "source": [
    "# Create a  __init__.py file that will contain that will be used to run our functions. \n",
    "# This file will be stored in our pwd (/content/tests)\n",
    "%%file __init__.py\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nc0JGQHody5D"
   },
   "outputs": [],
   "source": [
    "# Create a bank_data.py file that will contain the import_data function. \n",
    "# This file will be stored in our pwd (/content/tests).\n",
    "%%file stock_data.py\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "def import_data():\n",
    "  url = \"https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/SP_500_5yr_stock_data.csv\"\n",
    "  df = pd.read_csv(url)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7jfvswveA5W"
   },
   "outputs": [],
   "source": [
    "%%file test_stock_data.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEdynwDbiO8h"
   },
   "outputs": [],
   "source": [
    "!python -m pytest test_stock_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ih5d0yBaWg8"
   },
   "source": [
    "## Question 10\n",
    "\n",
    "- **8 points**\n",
    "\n",
    "Read in the [SP_500_5yr_stock_data.csv](\"https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/SP_500_5yr_stock_data.csv) dataset into Spark DataFrame called, `stock_df`, then import the Spark DataFrame into a Great Expectations DataFrame called, `stock_df_ge = ge.dataset.SparkDFDataset(stock_df)`, and answer the following question:\n",
    "\n",
    "What is the code that will test that the number of columns is \"6\" and pass when executed?\n",
    "\n",
    "a. `assert stock_df_ge.column_count() == 6`\n",
    "\n",
    "b. `print(stock_df_ge.expect_column_count_to_equal(6))`\n",
    "\n",
    "c. `print(stock_df_ge.expect_column_count_to_equal(6))`\n",
    "\n",
    "d. `print(stock_df_ge.expect_table_column_count_to_equal(6))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6D8AaFO4n1oh"
   },
   "outputs": [],
   "source": [
    "# Install great expectations\n",
    "!pip install great_expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srB4N3ZaqBCQ"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "import great_expectations as ge\n",
    "spark = SparkSession.builder.appName(\"Vehicles\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhLmF4qMrxg7"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/nflx-data-science-adv/week-6/SP_500_5yr_stock_data.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "stock_df = spark.read.csv(SparkFiles.get(\"SP_500_5yr_stock_data.csv\"), header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "stock_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVwnLtY8tAC1"
   },
   "outputs": [],
   "source": [
    "# Test that the number of columns is \"6\" and passes when executed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MqJl3i5tHcI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Big_Data_Spark_Assessment.ipynb",
   "provenance": []
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
    "theme": {
     "0535adbc-b74f-46cc-9cd6-4eabe2477c8e": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         43,
         43,
         43
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         238,
         238,
         238
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         19,
         218,
         236
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         238,
         238,
         238
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     },
     "cc59980f-cb69-400a-b63a-1fb85ca73c8a": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "cc59980f-cb69-400a-b63a-1fb85ca73c8a",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  },
  "nteract": {
   "version": "0.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "df07ec09c4017cff5cffe5c6e95982b3fd889b25c9af2c66175cc1127f95fbfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
